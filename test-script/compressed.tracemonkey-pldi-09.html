<?xml version="1.0" encoding="UTF-8" ?>
<!-- Created from PDF via Acrobat SaveAsXML -->
<!-- Mapping Table version: 28-February-2003 -->
<TaggedPDF-doc>
<?xpacket begin='﻿' id='W5M0MpCehiHzreSzNTczkc9d'?>
<?xpacket begin="﻿" id="W5M0MpCehiHzreSzNTczkc9d"?>
<x:xmpmeta xmlns:x="adobe:ns:meta/" x:xmptk="Adobe XMP Core 5.2-c001 63.139439, 2010/09/27-13:37:26        ">
   <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
      <rdf:Description rdf:about=""
            xmlns:xmp="http://ns.adobe.com/xap/1.0/">
         <xmp:CreateDate>2009-04-01T16:39:25-07:00</xmp:CreateDate>
         <xmp:CreatorTool>TeX</xmp:CreatorTool>
      </rdf:Description>
      <rdf:Description rdf:about=""
            xmlns:pdfx="http://ns.adobe.com/pdfx/1.3/">
         <pdfx:PTEX.Fullbanner>This is pdfeTeX, Version 3.141592-1.21a-2.2 (Web2C 7.5.4) kpathsea version 3.5.6</pdfx:PTEX.Fullbanner>
      </rdf:Description>
      <rdf:Description rdf:about=""
            xmlns:pdf="http://ns.adobe.com/pdf/1.3/">
         <pdf:Producer>pdfeTeX-1.21a</pdf:Producer>
      </rdf:Description>
      <rdf:Description rdf:about=""
            xmlns:dc="http://purl.org/dc/elements/1.1/">
         <dc:format>xml</dc:format>
      </rdf:Description>
   </rdf:RDF>
</x:xmpmeta>
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                                                                                                    
                           
<?xpacket end="w"?>
<?xpacket end='r'?>

<Part>
<H2>Trace-based Just-in-Time Type Specialization for Dynamic 
Languages 
</H2>

<P>Andreas Gal∗+, Brendan Eich∗, Mike Shaver∗, David Anderson∗, David Mandelin∗ , 
Mohammad R. Haghighat$, Blake Kaplan∗, Graydon Hoare∗, Boris Zbarsky∗, Jason Orendorff∗ , 
Jesse Ruderman∗, Edwin Smith#, Rick Reitmaier#, Michael Bebenita+, Mason Chang+#, Michael Franz+ 
</P>

<P>Mozilla Corporation∗ </P>

<P>{gal,brendan,shaver,danderson,dmandelin,mrbkap,graydon,bz,jorendorff,jruderman}@mozilla.com </P>

<P>Adobe Corporation# </P>

<P>{edwsmith,rreitmai}@adobe.com </P>

<P>Intel Corporation$ </P>

<P>{mohammad.r.haghighat}@intel.com </P>

<P>University of California, Irvine+ </P>

<P>{mbebenit,changm,franz}@uci.edu </P>

<Sect>
<P>Abstract </P>

<P>Dynamic languages such as JavaScript are more difﬁcult to compile than statically typed ones. Since no concrete type information isavailable, traditional compilers needto emit generic code that can handle all possible type combinationsat runtime.We present an alternative compilation technique for dynamically-typed languages that identiﬁes frequently executed loop traces at run-time and then generates machine code on the ﬂy that is specialized for the actual dynamic types occurring on each path through the loop. Our method provides cheap inter-procedural type specialization, and an elegant and efﬁcient way of incrementally compiling lazily discovered alternative paths through nested loops.Wehave implemented a dynamic compiler for JavaScript based on our technique and we have measured speedups of 10x and more for certain benchmark programs. </P>

<P>Categories and Subject Descriptors D.3.4[Programming Languages]: Processors — Incremental compilers, codegeneration. </P>

<P>General Terms Design, Experimentation, Measurement, Performance. </P>

<P>Keywords JavaScript, just-in-time compilation, trace trees. </P>

<P>1. Introduction </P>

<P>Dynamic languages such as JavaScript, Python, and Ruby, are popular since they are expressive, accessible to non-experts, and make deployment as easy as distributing a source ﬁle. They are used for small scripts as well as for complex applications. JavaScript, for example,is thedefacto standard for client-side web programming </P>

<P>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. </P>

<P>PLDI’09, June 15–20, 2009, Dublin, Ireland. </P>

<P>and is used for the application logic of browser-based productivity applications such as Google Mail, Google Docs and Zimbra Collaboration Suite. In this domain, in order to provide a ﬂuid user experienceand enableanewgenerationof applications, virtual machines must provide a low startup time and high performance. </P>

<P>Compilers for statically typed languages rely on type informationto generateefﬁcient machine code.Inadynamically typed programming language such as JavaScript, the types of expressions may vary at runtime. This means that the compiler can no longer easily transform operations into machine instructions that operate on one speciﬁc type.Withoutexact type information, the compiler must emit slower generalized machine code that can deal with all potential type combinations. While compile-time static type inference might be able to gather type information to generate optimized machine code, traditional static analysis is very expensive and hence not well suited for the highly interactive environment of a web browser. </P>

<P>We present a trace-based compilation technique for dynamic languages that reconciles speed of compilation with excellent performanceofthe generated machine code.Our system usesamixedmodeexecution approach:the system starts runningJavaScriptina fast-starting bytecode interpreter. As the program runs, the system identiﬁes hot (frequently executed) bytecode sequences, records them, and compiles them to fast native code. We call such a sequence of instructions a trace. </P>

<P>Unlike method-based dynamic compilers, our dynamic compiler operates at the granularity of individual loops. This design choice is based on the expectation that programs spend most of their time in hot loops. Even in dynamically typed languages, we expecthotloopstobe mostlytype-stable,meaningthatthe typesof values areinvariant.(12)Forexample,wewouldexpectloop counters that start as integers to remain integers for all iterations. When both of these expectations hold, a trace-based compiler can cover the programexecutionwitha small numberof type-specialized,efﬁciently compiled traces. </P>

<P>Each compiled trace covers one path through the program with onemappingofvaluestotypes.WhentheVMexecutesacompiled trace, it cannot guarantee that the same path will be followed </P>

<P>c</P>
</Sect>

<P>Copyright © 2009ACM 978-1-60558-392-1/09/06... $5.00 or that the same types will occur in subsequent loop iterations. </P>

<Sect>
<P>Hence, recording and compilingatrace speculates that the path and typingwillbeexactlyastheywereduring recordingfor subsequent iterations of the loop. </P>

<P>Every compiled trace contains all the guards (checks) required to validate the speculation. If one of the guards fails (if control ﬂow is different, or a value of a different type is generated), the trace exits. If an exit becomes hot, the VM can record a branch trace starting at the exit to cover the new path. In this way, the VM records a trace tree covering all the hot paths through the loop. </P>

<P>Nested loops can be difﬁcult to optimize for tracing VMs. In a na¨ıve implementation, inner loops would become hot ﬁrst, and the VM would start tracing there. When the inner loop exits, the VMwould detectthatadifferent branchwastaken.TheVMwould try to record a branch trace, and ﬁnd that the trace reaches not the inner loop header,but the outer loop header.At this point, theVM could continue tracing until it reaches the inner loop header again, thus tracing the outer loop inside a trace tree for the inner loop. But this requires tracing a copyof the outer loop for every side exit and type combination in the inner loop. In essence, this is a form of unintended tail duplication, which can easily overﬂow the code cache. Alternatively,theVMcouldsimplystoptracing,andgiveup on ever tracing outer loops. </P>

<P>We solve the nested loop problem by recording nested trace trees. Our system traces the inner loopexactly as the na¨ıveversion. The system stops extending the inner tree when it reaches an outer loop,but thenit startsanew traceatthe outerloop header. When the outer loop reaches the inner loop header, the system tries to call the tracetree for the inner loop. If the call succeeds, the VM records the call to the inner tree as part of the outer trace and ﬁnishes the outer trace as normal. In this way, our system can trace any number of loops nested to anydepth without causing excessive tail duplication. </P>

<P>These techniques allow a VM to dynamically translate a program to nested, type-specialized trace trees. Because traces can cross function call boundaries, our techniques also achieve the effects of inlining. Because traces haveno internal control-ﬂowjoins, they can be optimized in linear time by a simple compiler (10). Thus, our tracing VM efﬁciently performs the same kind of optimizations that would require interprocedural analysis in a static optimization setting. This makes tracing an attractive and effective tool to type specialize even complex function call-rich code. </P>

<P>We implemented these techniques for an existing JavaScript interpreter, SpiderMonkey.We call the resulting tracing VM TraceMonkey.TraceMonkeysupports all the JavaScript features of SpiderMonkey, with a 2x-20x speedup for traceable programs. </P>

<P>This paper makes the following contributions: </P>

<L>
<LI>
<LI_Label>• </LI_Label>

<LI_Title>We explain an algorithm for dynamically forming trace trees to 
coveraprogram, representing nested loops as nested trace trees. 
</LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>Weexplainhowto speculatively generateefﬁcient type-specialized code for traces from dynamic language programs. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>We validate our tracing techniques in an implementation based 
on the SpiderMonkeyJavaScript interpreter, achieving 2x-20x 
speedups on manyprograms. 
</LI_Title>
</LI>
</L>

<P>The remainderofthispaperisorganizedasfollows. Section3is a generaloverviewof tracetree based compilationwe useto capture and compile frequently executed code regions. In Section 4 we describe our approach of covering nested loops using a number of individual trace trees. In Section 5 we describe our trace-compilation based speculative type specialization approach we use to generate efﬁcient machine code from recorded bytecode traces. Our implementation of a dynamic type-specializing compiler for JavaScript is described in Section 6. Related work is discussed in Section8.In Section7weevaluate our dynamic compiler basedon </P>

<P>1 for (var i = 2; i < 100; ++i) { 
2 if (!primes[i]) 
3 continue; 
4 for(vark=i+i;i&lt;100;k+=i) 
5 primes[k] = false; 
6} 
</P>

<P>Figure 1. Sample program: sieve of Eratosthenes. primes is initialized to an array of 100 false values on entry to this code snippet. </P>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_0.jpg"/>
Interpret BytecodesMonitor RecordLIR TraceExecute Compiled TraceEnter Compiled TraceCompileLIR TraceLeave Compiled Traceloop edgehotloop/exitabort recordingfinish at loop headercold/blacklistedloop/exitcompiled trace readyloop edge with same typesside exit to existing traceside exit,no existing traceOverhead InterpretingNativeSymbol Key</Figure>

<P>Figure 2. State machine describing the major activities ofTrace-Monkey and the conditions that cause transitions to a new activity. In the dark box, TM executes JS as compiled traces. In the light gray boxes, TM executes JS in the standard interpreter. White boxes are overhead. Thus, to maximize performance, we need to maximize time spent in the darkest box and minimize time spent in the white boxes. The best case is a loop where the types at the loop edge are the same as the types on entry–then TM can stay in native code until the loop is done. </P>

<P>a set of industry benchmarks. The paper ends with conclusions in Section9andan outlookon futureworkis presentedin Section10. </P>

<P>2. Overview: Example Tracing Run </P>

<P>This section provides an overview of our system by describing how TraceMonkey executes an example program. The example program, shown in Figure 1, computes the ﬁrst 100 prime numbers withnestedloops.The narrativeshouldbereadalongwithFigure2, which describes the activitiesTraceMonkeyperforms and when it transitions between the loops. </P>

<P>TraceMonkey always begins executing a program in the byte-code interpreter. Every loop back edge is a potential trace point. When the interpreter crosses a loop edge, TraceMonkey invokes the trace monitor, which may decide to record or execute a native trace. At the start of execution, there are no compiled traces yet, so the trace monitor counts the number of times each loop back edge is executed untila loop becomes hot,currently after2crossings. Note thatthewayourloopsare compiled,theloopedgeis crossed before entering the loop, so the second crossing occurs immediately after the ﬁrst iteration. </P>

<P>Here is the sequence of events broken down by outer loop iteration: </P>
</Sect>

<P>v0 := ld state[748] // load primes from the trace activation record </P>

<P>st sp[0], v0 // store primes to interpreter stack v1 := ld state[764] // load k from the trace activation record v2 := i2f(v1) // convert k from int to double </P>

<P>st sp[8], v1 // store k to interpreter stack </P>

<P>st sp[16], 0 // store false to interpreter stack v3 := ld v0[4] // load class word for primes v4 := and v3, -4 // mask out object class tag for primes v5 := eq v4, Array // test whether primes is an array </P>

<P>xf v5 // side exit if v5 is false v6 := js_Array_set(v0, v2, false) // call function to set array element v7 := eq v6, 0 // test return value from call </P>

<P>xt v7 // side exit if js_Array_set returns false. </P>

<P>Figure 3. LIR snippet for sample program. This is the LIR recorded for line5 of the sample program in Figure 1. The LIR encodes the semantics in SSA form using temporary variables. The LIR also encodes all the stores that the interpreter would do to its data stack. Sometimes these stores can be optimized away as the stack locations are live only on exits to the interpreter. Finally, the LIR records guards and side exits to verify the assumptions made in this recording: that primes is an array and that the call to set its element succeeds. </P>

<P>mov edx, ebx(748) // load primes from the trace activation record </P>

<P>mov edi(0), edx // (*) store primes to interpreter stack </P>

<P>mov esi, ebx(764) // load k from the trace activation record </P>

<P>mov edi(8), esi // (*) store k to interpreter stack </P>

<P>mov edi(16), 0 // (*) store false to interpreter stack </P>

<P>mov eax, edx(4) // (*) load object class word for primes </P>

<P>and eax, -4 // (*) mask out object class tag for primes </P>

<P>cmp eax, Array // (*) test whether primes is an array </P>

<P>jne side_exit_1 // (*) side exit if primes is not an array </P>

<P>sub esp, 8 // bump stack for call alignment convention </P>

<Sect>
<P>push false // push last argument for call </P>
</Sect>

<P>push esi // push first argument for call </P>

<P>call js_Array_set // call function to set array element </P>

<Sect>
<P>add esp, 8 // clean up extra stack space </P>
</Sect>

<P>mov ecx, ebx // (*) created by register allocator </P>

<P>test eax, eax // (*) test return value of js_Array_set </P>

<P>je side_exit_2 // (*) side exit if call failed </P>

<Sect>
<P>... </P>

<P>side_exit_1: </P>

<P>mov ecx, ebp(-4) // restore ecx </P>

<P>mov esp, ebp // restore esp </P>

<P>jmp epilog // jump to ret statement </P>
</Sect>

<P>Figure 4. x86 snippet for sample program. This is the x86 code compiled from the LIR snippet in Figure 3. Most LIR instructions compile to a single x86 instruction. Instructions marked with (*) would be omitted by an idealized compiler that knew that none of the side exits wouldeverbe taken. The17 instructions generatedby the compiler comparefavorably with the 100+ instructions that the interpreterwould </P>

<Sect>
<P>execute for the same code snippet, including4indirect jumps. </P>

<P>i=2. This is the ﬁrst iteration of the outer loop. The loop on lines 4-5 becomes hot on its second iteration, soTraceMonkey enters recording mode on line 4. In recording mode, TraceMonkey records the code along the trace in a low-level compiler intermediate representation we call LIR. The LIR trace encodes all the operations performed and the types of all operands. The LIR trace also encodes guards, which are checks that verify that the control ﬂow and types are identical to those observed during trace recording. Thus, on later executions, if and only if all guards are passed, the trace has the required program semantics. </P>

<P>TraceMonkey stops recording when execution returns to the loop header or exits the loop. In this case, execution returns to the loop header on line 4. </P>

<P>After recording is ﬁnished,TraceMonkeycompiles the trace to native code using the recorded type information for optimization. The result is a native code fragment that can be entered if the interpreter PC and the types of values match those observed when trace recording was started. The ﬁrst trace in our example, T45, coverslines4and5.This tracecanbe enteredifthePCisatline4, i and k are integers, and primes is an object. After compiling T45, TraceMonkeyreturns to the interpreter and loops back to line 1. </P>

<P>i=3. Now the loop header at line1 has become hot, soTrace-Monkey starts recording. When recording reaches line 4, Trace-Monkeyobserves that it has reached an inner loop header that already hasa compiled trace, soTraceMonkey attempts to nest the inner loop inside the current trace. The ﬁrst step is to call the inner traceasasubroutine.Thisexecutesthelooponline4to completion andthen returnstothe recorder.TraceMonkey veriﬁesthatthecall was successful and then records the call to the inner trace as part of the current trace. Recording continues until execution reaches line 1, and at which pointTraceMonkeyﬁnishes and compilesa trace for the outer loop, T16. </P>

<P>i=4. On this iteration,TraceMonkeycalls T16. Because i=4,the if statement on line2is taken. This branchwas not takenin the original trace, so this causes T16 tofaila guard andtakea sideexit. Theexitisnotyethot,soTraceMonkey returnstothe interpreter, which executes the continue statement. </P>

<P>i=5. TraceMonkeycallsT16, which in turn calls the nested trace T45. T16 loops back to its own header, starting the next iteration without ever returning to the monitor. </P>

<P>i=6. On this iteration, the sideexit on line2is takenagain. This time, the side exit becomes hot, so a trace T23,1 is recorded that coversline3and returnstotheloop header.Thus,theendof T23,1 jumps directly to the start of T16. The side exit is patched so that on future iterations, it jumps directly to T23,1. </P>

<P>Atthispoint,TraceMonkeyhas compiledenough tracestocover the entire nested loop structure, so the rest of the program runs entirely as native code. </P>

<P>3. Trace Trees </P>

<P>In this section, we describe traces, trace trees, and how they are formed at run time. Although our techniques apply to anydynamic language interpreter, we will describe them assuming a bytecode interpretertokeeptheexposition simple. </P>

<Sect>
<H6>3.1 Traces </H6>

<P>A trace is simply a program path, which may cross function call boundaries.TraceMonkeyfocuses on loop traces, that originate at a loop edge and represent a single iteration through the associated loop. </P>

<P>Similar to an extended basic block, a trace is only entered at thetop,butmayhave many exits.In contrasttoanextended basic block, a trace can contain join nodes. Since a trace always only follows one single path through the original program, however,join nodes are not recognizable as such in a trace and have a single predecessor node like regular nodes. </P>

<P>Atyped trace is a trace annotated with a type for every variable (including temporaries) on the trace.Atyped trace also has an entry type map giving the required types for variables used on the trace beforetheyare deﬁned.Forexample,atracecouldhaveatypemap </P>

<P>(x: int, b: boolean), meaning that the trace may be entered only if the value of the variable x is of type int and the value of b is of type boolean. The entry type map is much like the signature of a function. </P>

<P>In this paper, we only discuss typed loop traces, and we will refer to them simply as “traces”. Thekey property of typed loop traces is that they can be compiled to efﬁcient machine code using the same techniques used for typed languages. </P>

<P>InTraceMonkey, traces are recordedin trace-ﬂavored SSA LIR (low-level intermediate representation). In trace-ﬂavored SSA (or TSSA), phi nodes appear only at the entry point, which is reached both on entry and via loop edges. The important LIR primitives are constant values, memory loads and stores (by address and offset), integer operators, ﬂoating-point operators, function calls, and conditionalexits.Type conversions, suchas integerto double, are represented by function calls. This makes the LIR used by TraceMonkey independent of the concrete type system and type conversion rules of the source language. The LIR operations are generic enough that the backend compiler is language independent. Figure3shows anexample LIR trace. </P>

<P>Bytecode interpreters typically represent values in a various complex data structures (e.g., hash tables) in a boxed format (i.e., with attached type tag bits). Sincea traceis intended to represent efﬁcient code that eliminates all that complexity, our traces operate on unboxed values in simple variables and arrays as much as possible. </P>

<P>Atrace records all its intermediate values in a small activation record area.To makevariable accessesfast on trace, the trace also imports local and global variables by unboxing them and copying them to its activation record. Thus, the trace can read and write thesevariables withsimple loads and stores fromanativeactivation recording, independently of the boxing mechanism used by the interpreter. When the trace exits, the VM boxes the values from this native storage location and copies them back to the interpreter structures. </P>

<P>For every control-ﬂow branch in the source program, the recorder generates conditionalexit LIR instructions. These instructions exit from the trace if required control ﬂow is different from what it was at trace recording, ensuring that the trace instructions are run only if they are supposed to. We call these instructions guard instructions. </P>

<P>Most of our traces represent loops and end with the special loop LIR instruction. This is just an unconditional branch to the top of the trace. Such traces return only via guards. </P>

<P>Now, we describe thekey optimizations that are performed as part of recording LIR. All of these optimizations reduce complex dynamic language constructs to simple typed constructs by specializing for the current trace. Each optimization requires guard instructions to verify their assumptions about the state and exit the trace if necessary. </P>

<P>Type specialization. </P>

<P>All LIR primitives apply to operands of speciﬁc types. Thus, LIR traces are necessarily type-specialized, and a compiler can easily produce a translation that requires no type dispatches. A typical bytecode interpreter carries tag bits along with each value, and to perform anyoperation, must check the tag bits, dynamically dispatch, mask out the tag bits to recover the untagged value, perform the operation, and then reapplytags. LIR omits everything except the operation itself. </P>

<P>Apotential problem is that some operations can produce values of unpredictable types. For example, reading a property from an object could yield a value of any type, not necessarily the type observed during recording. The recorder emits guard instructions that conditionally exit if the operation yields a value of a different type from that seen during recording. These guard instructions guarantee that as long as execution is on trace, the types of values match those of the typed trace. When the VM observes a side exit along such a type guard, a new typed trace is recorded originating at the side exit location, capturing the new type of the operation in question. </P>

<P>Representation specialization: objects. In JavaScript, name lookup semantics are complex and potentially expensive because theyinclude features like object inheritance and eval.To evaluate an object property read expression like o.x, the interpreter must search the property map of o and all of its prototypes and parents. Property maps can be implemented with different data structures (e.g., per-object hash tables or shared hash tables), so the search process also must dispatch on the representation of each object found during search.TraceMonkeycan simply observethe resultof the search process and record the simplest possible LIR to access the propertyvalue.Forexample,the searchmightﬁndsthevalueof </P>

<P>o.x in the prototype of o, which uses a shared hash-table representation that places x inslot2ofa propertyvector.Thenthe recorded can generate LIR that reads o.x with just two or three loads: one to getthe prototype, possibly onetogetthe propertyvaluevector,and one moretogetslot2fromthevector.Thisisavast simpliﬁcation and speedup compared to the original interpreter code. Inheritance relationships and object representations can change during execution, so the simpliﬁed code requires guard instructions that ensure theobject representationisthe same.InTraceMonkey,objects’representations are assigned an integer key called the object shape. Thus, the guard is a simple equality check on the object shape. </P>

<P>Representation specialization: numbers. JavaScript has no integer type, only a Number type that is the set of 64-bit IEEE754 ﬂoating-pointer numbers (“doubles”). But many JavaScript operators, in particular array accesses and bitwise operators, really operate on integers, so theyﬁrst convert the number to an integer, and then convert any integer result back to a double.1 Clearly, a JavaScriptVM thatwantstobefast must ﬁndawayto operateon integers directly and avoid these conversions. </P>

<P>InTraceMonkey, we support two representations for numbers: integers and doubles. The interpreter uses integer representations as much as it can, switching for results that can only be represented as doubles. When a trace is started, some values may be imported and represented as integers. Some operations on integers require guards.Forexample, addingtwo integers can produceavalue too large for the integer representation. </P>

<P>Function inlining. LIR traces can cross function boundaries in either direction, achieving function inlining. Move instructions need to be recorded for function entry and exit to copy arguments in and returnvalues out. These move statements are then optimized away by the compiler using copypropagation. In order to be able to return to the interpreter, the trace must also generate LIR to record that a call frame has been entered and exited. The frame entry and exit LIR saves just enough information to allow the intepreter call stack to be restored later and is much simpler than the interpreter’s standard call code. If the function being entered is not constant (which in JavaScript includes anycall by function name), the recorder must also emit LIR to guard that the function is the same. </P>

<P>Guards and side exits. Each optimization described above requires one or more guards to verify the assumptions made in doing the optimization.Aguardis justa groupof LIR instructions that performs a test and conditional exit. The exit branches to a side exit, a small off-trace piece of LIR that returns a pointer to a structure that describes the reason for the exit along with the interpreter PC at the exit point and anyother data needed to restore the interpreter’s state structures. </P>

<P>Aborts. Some constructs are difﬁcult to record in LIR traces. For example, eval or calls to external functions can change the program state in unpredictable ways, making it difﬁcult for the tracer to know the current type map in order to continue tracing. Atracing implementation can also have anynumber of other limitations, e.g.,a small-memory device may limit the length of traces. When anysituation occurs that prevents the implementation from continuing trace recording, the implementation aborts trace recording and returns to the trace monitor. </P>
</Sect>

<Sect>
<H6>3.2 Trace Trees </H6>

<P>Especially simple loops, namely those where control ﬂow, value types, value representations, and inlined functions are all invariant, can be represented by a single trace. But most loops have at least some variation, and so the program will take side exits from the main trace. Whena sideexit becomes hot,TraceMonkey startsa new branchtrace from that point and patches the side exit to jump directlytothat trace.Inthisway,asingle traceexpandson demand to a single-entry, multiple-exit trace tree. </P>

<P>This section explains how trace trees are formed during execution. The goal is to form trace trees during execution that cover all the hot paths of the program. </P>

<P>1Arrays are actually worse than this: if the index value is a number, it must be converted from a double to a string for the property access operator, and then to an integer internally to the array implementation. </P>

<P>Starting a tree. Tree trees always start at loop headers, because theyarea naturalplacetolookforhotpaths.InTraceMonkey,loop headers are easy to detect–the bytecode compiler ensures that a bytecode is a loop header iffit is the target of a backward branch. TraceMonkeystarts a tree when a given loop header has been executed a certain number of times (2 in the current implementation). Starting a tree just means starting recording a trace for the current pointandtypemapandmarkingthe traceastherootofa tree.Each treeis associatedwithaloop headerandtypemap,so theremaybe several treesforagivenloop header. </P>

<P>Closing the loop. Trace recording can end in several ways. </P>

<P>Ideally, the trace reaches the loop header where it started with the same type map as on entry. This is called a type-stable loop iteration. In this case, the end of the trace can jump right to the beginning, as all the value representations are exactly as needed to enter the trace. The jump can even skip the usual code that would copyout the state at the end of the trace and copyit back in to the trace activation record to enter a trace. </P>

<P>In certain cases the trace might reach the loop header with a different type map. This scenario is sometime observed for the ﬁrst iterationofaloop.Somevariables insidetheloopmight initiallybe undeﬁned,beforetheyaresettoaconcretetypeduringtheﬁrstloop iteration. When recording such an iteration, the recorder cannot link the trace back to its own loop header since it is type-unstable. Instead, the iteration is terminated with a side exit that will always fail and return to the interpreter. At the same time a new trace is recorded with the new type map. Every time an additional type-unstable traceisaddedtoaregion,itsexittypemapis comparedto the entry map of all existing traces in case theycomplement each other.With this approach we are able to cover type-unstable loop iterations as long theyeventually form a stable equilibrium. </P>

<P>Finally, the trace might exit the loop before reaching the loop header, for example because execution reaches a break or return statement. In this case, the VM simply ends the trace with an exit to the trace monitor. </P>

<P>As mentioned previously, we may speculatively chose to represent certain Number-typedvaluesas integerson trace.Wedoso when we observe that Number-typed variables contain an integer value at trace entry. If during trace recording the variable is unexpectedly assigned a non-integer value, we have to widen the type of the variable to a double. As a result, the recorded trace becomes inherently type-unstable since it starts with an integer value but ends with a double value. This represents a mis-speculation, since at trace entry we specialized the Number-typed value to an integer, assuming thatattheloopedgewewouldagain ﬁndan integervalue inthevariable,allowingustoclosetheloop.Toavoid futurespeculativefailuresinvolvingthisvariable,andto obtaina type-stable tracewe notethefactthatthevariablein questionasbeen observed to sometimes hold non-integer values in an advisory data structure which we call the “oracle”. </P>

<P>When compiling loops, we consult the oracle before specializing values to integers. Speculation towards integers is performed only if no adverse information is known to the oracle about that particular variable. Whenever we accidentally compile a loop that is type-unstable due to mis-speculation of a Number-typed variable, we immediately trigger the recording of a new trace, which based on the now updated oracle information will start with a double value and thus become type stable. </P>

<P>Extending a tree. Side exits lead to different paths through the loop, or paths with different types or representations. Thus, to completely cover the loop, the VM must record traces starting at all side exits. These traces are recorded much like root traces: there is acounter for each sideexit, and when the counter reachesahotness threshold, recording starts. Recording stops exactly as for the root trace, using the loop header of the root trace as the target to reach. </P>

<P>Our implementation does not extend at all side exits. It extends onlyifthesideexitisfora control-ﬂow branch,andonlyiftheside exit does not leave the loop. In particular we do not want to extend a trace tree along a path that leads to an outer loop, because we want to cover such paths in an outer tree through tree nesting. </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H6>3.3 Blacklisting </H6>

<P>Sometimes, a program follows a path that cannot be compiled into a trace, usually because of limitations in the implementation. TraceMonkey does not currently support recording throwing and catching of arbitrary exceptions. This design trade off was chosen, because exceptions are usually rare in JavaScript. However, if a program opts to use exceptions intensively, we would suddenly incur a punishing runtime overhead if we repeatedly try to record a trace for this path and repeatedly fail to do so, since we abort tracing every time we observe an exception being thrown. </P>

<P>Asa result,ifahotloop contains tracesthatalwaysfail,theVM could potentially run much more slowly than the base interpreter: theVM repeatedly spendstime tryingto record traces,butisnever ableto runany.Toavoid this problem, whenevertheVMis about to start tracing, it must try to predict whether it will ﬁnish the trace. </P>

<P>Our prediction algorithm is based on blacklisting traces that havebeentriedandfailed.WhentheVMfailsto ﬁnishatrace startingatagivenpoint,theVM recordsthatafailurehas occurred.The VMalsosetsacountersothatitwillnottrytorecordatracestarting at that point until it is passed a few more times (32 in our implementation). This backoff counter gives temporary conditions that prevent tracinga chance to end.Forexample,a loop may behave differently during startup than during its steady-stateexecution. Afteragiven numberoffailures(2in our implementation), theVM marks the fragment as blacklisted, which means the VM will never again start recording at that point. </P>

<P>After implementing this basic strategy, we observed that for small loops that get blacklisted, the system can spend a noticeable amount of time just ﬁnding the loop fragment and determining that ithasbeen blacklisted.Wenowavoidthatproblemby patchingthe bytecode.We deﬁne anextra no-op bytecode that indicatesa loop header. The VM calls into the trace monitor every time the interpreterexecutesa loop header no-op.To blacklist a fragment, we simply replace the loop header no-op with a regular no-op. Thus, the interpreter will never again even call into the trace monitor. </P>

<P>Thereisarelated problemwehavenotyetsolved, which occurs when a loop meets all of these conditions: </P>

<L>
<LI>
<LI_Label>• </LI_Label>

<LI_Title>The VM can form at least one root trace for the loop. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>There is at least one hot side exit for which the VM cannot complete a trace. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>The loop body is short. </LI_Title>
</LI>
</L>

<P>In this case, the VM will repeatedly pass the loop header,search for a trace, ﬁnd it, execute it, and fall back to the interpreter. With a short loop body, the overhead of ﬁnding and calling the trace is high, and causes performance to be even slower than the basic interpreter. So far, in this situation we have improved the implementation so that the VM can complete the branch trace. But it is hard to guarantee that this situation will never happen. As future work, this situation could be avoided by detecting and blacklisting loops for which the average trace call executes few bytecodes before returning to the interpreter. </P>

<P>4. Nested Trace Tree Formation </P>

<P>Figure7shows basic trace tree compilation(11) appliedtoa nested loop where the inner loop contains two paths. Usually, the inner loop (with header at i2)becomes hot ﬁrst, and a trace tree is rooted atthatpoint.Forexample,theﬁrst recorded tracemaybeacycle Figure 5. A tree with two traces, a trunk trace and one branch trace. The trunk trace contains a guard to which a branch trace was attached.The branch trace containaguardthatmayfailand trigger asideexit.Boththetrunkandthe branch traceloopbacktothetree anchor, which is the beginning of the trace tree. </P>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_1.jpg"/>
TTrunk TraceTree AnchorTrace AnchorBranch TraceGuardSide Exit</Figure>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_2.jpg"/>
Trace 2Trace 1Trace 2Trace 1ClosedLinkedLinkedLinkedNumberNumberStringStringStringStringBooleanTrace 2Trace 1Trace 3LinkedLinkedLinkedClosedNumberNumberNumberBooleanNumberBooleanNumberBoolean(a)(b)(c)</Figure>

<P>Figure 6. We handle type-unstable loops by allowing traces to compile that cannot loop back to themselves due to a type mismatch. As such traces accumulate, we attempt to connect their loop edges to form groups of trace trees that can execute without having to side-exit to the interpreter to cover odd type cases. This is particularly important for nested trace trees where an outer tree tries to call an inner tree (or in this case a forest of inner trees), since inner loops frequently have initially undeﬁned values which change type to a concrete value after the ﬁrst iteration. </P>

<P>through the inner loop, {i2,i3,i5,α}. The α symbol is used to indicate that the trace loops back the tree anchor. </P>

<P>When execution leaves the inner loop, the basic design has two choices.First,thesystemcanstoptracingandgiveupon compiling the outer loop, clearly an undesirable solution. The other choice is to continue tracing, compiling traces for the outer loop inside the inner loop’s trace tree. </P>

<P>For example, the program might exit ati5 and record a branch trace that incorporates the outer loop: {i5,i7,i1,i6,i7,i1,α}. Later, the program might take the other branch at i2 and then exit, recording another branch trace incorporating the outer loop: {i2,i4,i5,i7,i1,i6,i7,i1,α}. Thus, the outer loop is recorded and compiled twice, and both copies must be retained in the trace cache. </P>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_3.jpg"/>
i2i3i4i5i1i6i7t1t2Tree CallOuter TreeNested TreeExit Guard(a)(b)</Figure>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_4.jpg"/>
i2i3i1i6i4i5t2t1t4Exit GuardNested Tree</Figure>

<P>Figure 7. Control ﬂow graph of a nested loop with an if statement inside the inner most loop (a). An inner tree captures the inner loop, and is nested inside an outer tree which “calls” the inner tree. The inner tree returns to the outer tree once it exits along its loop condition guard (b). </P>

<P>In general, if loops are nested to depth k, and each loop has n paths (on geometric average), this na¨ıve strategy yields O(n k) traces, which can easily ﬁll the trace cache. </P>

<P>In order to execute programs with nested loops efﬁciently, a tracing system needsatechnique for covering the nested loops with native code without exponential trace duplication. </P>

<Sect>
<H6>4.1 Nesting Algorithm </H6>

<P>Thekeyinsightis thatif eachloopis representedbyitsown trace tree, the code for each loop can be contained only in its own tree, and outerlooppathswillnotbe duplicated. Anotherkeyfactisthat we are not tracing arbitrary bytecodes that might have irreduceable control ﬂow graphs,but ratherbytecodes producedbya compiler for a language with structured control ﬂow. Thus, given two loop edges, the system can easily determine whether they are nested and which is the inner loop. Using this knowledge, the system can compile inner and outer loops separately,and makethe outer loop’s traces call the inner loop’s trace tree. </P>

<P>The algorithm forbuilding nested trace trees is as follows.We start tracing at loop headers exactly as in the basic tracing system. When we exit a loop (detected by comparing the interpreter PC with the range given by the loop edge), we stop the trace. The key step of the algorithm occurs when we are recording a trace for loop LR (R for loop being recorded) and we reach the header of a different loop LO (O for other loop). Note that LO must be an inner loop of LR because we stop the trace when we exit a loop. </P>

<L>
<LI>
<LI_Label>• </LI_Label>

<LI_Title>If LO has a type-matching compiled trace tree, we call LO as a nested trace tree. If the call succeeds, then we record the call in the trace for LR. On future executions, the trace for LR will call the inner trace directly. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>If LO does not have a type-matching compiled trace tree yet, we have to obtain it before we are able to proceed. In order to do this, we simply abort recording the ﬁrst trace. The trace monitor will see the inner loop header, and will immediately start recording the inner loop. 2 </LI_Title>
</LI>
</L>

<P>Ifalltheloopsinanestare type-stable,thenloopnesting creates no duplication. Otherwise,if loops are nestedtoadepth k,and each </P>

<P>2Instead of aborting the outer recording, we could principally merely suspendthe recording,but thatwould requirethe implementationtobe able to record several traces simultaneously, complicating the implementation, while saving only a few iterations in the interpreter. </P>

<P>Figure 8. Controlﬂowgraphofaloopwithtwo nestedloops(left) and its nested trace tree conﬁguration (right). The outer tree calls the two inner nested trace trees and places guards at their side exit locations. </P>

<P>loop is entered with m differenttype maps(on geometricaverage), then we compile O(m k) copies of the innermost loop. As long as m is close to 1, the resulting trace trees will be tractable. </P>

<P>An important detail is that the call to the inner trace tree must act like a function call site: it must return to the same point every time. The goal of nesting is to make inner and outer loops independent; thus when the inner tree is called, it must exit to the same point in the outer tree every time with the same type map. Because we cannot actually guarantee this property, we must guard on it after the call, and side exit if the property does not hold. A common reason for the inner tree not to return to the same point would be if the inner tree took a new side exit for which it had never compiled a trace. At this point, the interpreter PC is in the inner tree, so we cannot continue recording or executing the outer tree. If this happens during recording, we abort the outer trace, to give the inner treea chanceto ﬁnishgrowing.Afutureexecutionofthe outer tree would then be able to properly ﬁnish and record a call to the inner tree.If an inner tree sideexit happens duringexecutionof a compiled trace for the outer tree, we simply exit the outer trace and start recording a new branch in the inner tree. </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H6>4.2 Blacklisting with Nesting </H6>

<P>The blacklisting algorithm needs modiﬁcation to work well with nesting. The problem is that outer loop traces often abort during startup (because the inner tree is not available or takes a side exit), which would lead to their being quickly blacklisted by the basic algorithm. </P>

<P>Thekeyobservation is that when an outer trace aborts because the inner tree is not ready, this is probably a temporary condition. Thus, we should not count such aborts toward blacklisting as long asweareabletobuildup more tracesfortheinner tree. </P>

<P>In our implementation, when an outer tree aborts on the inner tree, we increment the outer tree’s blacklist counter as usual and back off on compiling it. When the inner tree ﬁnishes a trace, we decrement the blacklist counter on the outer loop, “forgiving” the outer loop for aborting previously.Wealso undo the backoffso that the outer tree can start immediately trying to compile the next time we reach it. </P>

<P>5. Trace Tree Optimization </P>

<P>This section explains how a recorded trace is translated to an optimized machine code trace. The trace compilation subsystem, NANOJIT, is separate from the VM and can be used for other applications. </P>

<Sect>
<H6>5.1 Optimizations </H6>

<P>Because traces are in SSA form and have no join points or φnodes, certain optimizations are easy to implement. In order to get good startup performance, the optimizations must run quickly, so we chose a small set of optimizations. We implemented the optimizations as pipelined ﬁlters so that they can be turned on and offindependently,andyetall runinjusttwoloop passesoverthe trace: one forward and one backward. </P>

<P>Every time the trace recorder emits a LIR instruction, the instruction is immediately passed to the ﬁrst ﬁlter in the forward pipeline. Thus, forward ﬁlter optimizations are performed as the trace is recorded. Each ﬁlter may pass each instruction to the next ﬁlter unchanged, write a different instruction to the next ﬁlter, or write no instruction at all.Forexample, the constant folding ﬁlter can replace a multiply instruction like v13 := mul3, 1000 with a constant instruction v13 = 3000. </P>

<P>We currently apply four forward ﬁlters: </P>

<L>
<LI>
<LI_Label>• </LI_Label>

<LI_Title>On ISAs without ﬂoating-point instructions, a soft-ﬂoat ﬁlter converts ﬂoating-point LIR instructions to sequences of integer instructions. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>CSE (constant subexpression elimination), </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>expression simpliﬁcation, including constant folding and a few algebraic identities (e.g., a − a =0), and </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>source language semantic-speciﬁc expression simpliﬁcation, primarily algebraic identitiesthatallow DOUBLEtobe replaced withINT.Forexample, LIR that converts an INT toa DOUBLE and then back again would be removed by this ﬁlter. </LI_Title>
</LI>
</L>

<P>When trace recording is completed, nanojit runs the backward optimization ﬁlters. These are used for optimizations that require backward program analysis. When running the backward ﬁlters, nanojitreadsoneLIR instructionatatime,andthereadsarepassed through the pipeline. </P>

<P>We currently apply three backward ﬁlters: </P>

<L>
<LI>
<LI_Label>• </LI_Label>

<LI_Title>Dead data-stack store elimination. The LIR trace encodes many stores to locations in the interpreter stack. But these values are never read back before exiting the trace (by the interpreter or another trace). Thus, stores to the stack that are overwritten before the next exit are dead. Stores to locations that are off the top of the interpreter stack at future exits are also dead. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>Dead call-stack store elimination. This is the same optimization as above, except applied to the interpreter’s call stack used for function call inlining. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>Dead code elimination. This eliminates any operation that stores to a value that is never used. </LI_Title>
</LI>
</L>

<P>After a LIR instruction is successfully read (“pulled”) from the backward ﬁlter pipeline, nanojit’s code generator emits native machine instruction(s) for it. </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H6>5.2 Register Allocation </H6>

<P>We use a simple greedy register allocator that makes a single backward pass over the trace (it is integrated with the code generator). By the time the allocator has reached an instruction like v3 = add v1,v2, it has already assigned a register to v3. If v1 and v2 have not yet been assigned registers, the allocator assigns a free register to each. If there are no free registers, a value is selected for spilling.We usea class heuristic that selects the “oldest” register-carried value (6). </P>

<P>The heuristic considers the set R of values v in registers immediately after the current instruction for spilling. Let vm be the last instruction before the current where each v is referred to. Then the Tag </P>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_5.jpg"/>
</Figure>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_6.jpg"/>
</Figure>

<P>JSType </P>

<P>Description xx1 </P>

<P>number </P>

<P>31-bit integer representation 000 </P>

<P>object </P>

<P>pointer to JSObject handle 010 </P>

<P>number </P>

<P>pointer to double handle 100 </P>

<P>string </P>

<P>pointer to JSString handle 110 </P>

<P>boolean </P>

<P>enumeration for null, undeﬁned, true,false null, or undeﬁned </P>

<P>Figure 9. Tagged values in the SpiderMonkey JS interpreter. </P>

<P>Testing tags, unboxing (extracting the untagged value) and boxing (creating taggedvalues) are signiﬁcant costs.Avoiding these costs isakeybeneﬁtof tracing. </P>

<P>heuristic selects v with minimum vm. The motivation is that this freesuparegisterforaslongas possiblegivena single spill. </P>

<P>If we need to spill a value vs at this point, we generate the restore code just after the code for the current instruction. The corresponding spill code is generated just after the last point where vs was used.Theregisterthatwas assignedto vs is marked free for the preceding code, because that register can now be used freely without affecting the following code </P>

<P>6. Implementation </P>

<P>To demonstrate the effectiveness of our approach, we have implemented a trace-based dynamic compiler for the SpiderMonkey JavaScript Virtual Machine (4). SpiderMonkey is the JavaScript VM embedded in Mozilla’s Firefox open-source web browser (2), whichisusedbymorethan200 million usersworld-wide.The core of SpiderMonkeyis a bytecode interpreter implemented in C++. </P>

<P>In SpiderMonkey, all JavaScript values are represented by the type jsval.A jsval is machinewordin whichuptothe3ofthe least signiﬁcant bits are a type tag, and the remaining bits are data. See Figure6for details. All pointers containedin jsvals point to GC-controlled blocks aligned on 8-byte boundaries. </P>

<P>JavaScript object values are mappings of string-valued property names to arbitrary values. Theyare represented in one of two ways in SpiderMonkey. Most objects are represented by a shared structural description, called the object shape,that maps property names to array indexes using a hash table. The object stores a pointer to the shape and the array of its own property values. Objects with large, unique sets of property names store their properties directly in a hash table. </P>

<P>Thegarbage collector is an exact, non-generational, stop-theworld mark-and-sweep collector. </P>

<P>Intherestofthis sectionwe discusskeyareasoftheTraceMonkeyimplementation. </P>

<Sect>
<H6>6.1 Calling Compiled Traces </H6>

<P>Compiled traces are stored in a trace cache, indexed by intepreter PC and type map. Traces are compiled so that they may be called as functions using standard native calling conventions (e.g., FASTCALL on x86). </P>

<P>The interpreter must hit a loop edge and enter the monitor in order to call a native trace for the ﬁrst time. The monitor computes the current type map, checks the trace cache for a trace for the current PC and type map, and if it ﬁnds one, executes the trace. </P>

<P>To execute a trace, the monitor must build a trace activation record containing imported local and global variables, temporary stack space, and space for arguments to native calls. The local and global values are then copied from the interpreter state to the trace activation record. Then, the traceis called likea normalCfunction pointer. </P>

<P>When a trace call returns, the monitor restores the interpreter state. First, the monitor checks the reason for the trace exit and applies blacklisting if needed. Then, it pops or synthesizes interpreter JavaScript call stack frames as needed. Finally, it copies the imported variables back from the trace activation record to the interpreter state. </P>

<P>At least in the current implementation, these steps have a non-negligible runtime cost, so minimizing the number of interpreterto-trace and trace-to-interpreter transitions is essential for performance. (see also Section 3.3). Our experiments (see Figure 12) show that for programs we can trace well such transitions happen infrequently and hence do not contribute signiﬁcantly to total runtime. In a few programs, where the system is prevented from recording branch traces for hot side exits by aborts, this cost can rise to up to 10% of total execution time. </P>
</Sect>

<Sect>
<H6>6.2 Trace Stitching </H6>

<P>Transitions from a trace to a branch trace at a side exit avoid the costs of calling traces from the monitor, in a feature called trace stitching. At a side exit, the exiting trace only needs to write live register-carriedvaluesbacktoits traceactivation record.Inourimplementation, identical type maps yield identical activation record layouts, so the trace activation record can be reused immediately by the branch trace. </P>

<P>In programs with branchy trace trees with small traces, trace stitching has a noticeable cost. Although writing to memory and then soon reading back would be expected to have a high L1 cache hit rate, for small traces the increased instruction count has a noticeable cost. Also, if the writes and reads are very close in the dynamic instruction stream, we have found that current x86 processors often incur penalties of 6 cycles or more (e.g., if the instructions use different base registers with equal values, the processor may not be able to detect that the addresses are the same right away). </P>

<P>The alternate solution is to recompile an entire trace tree, thus achieving inter-trace register allocation (10). The disadvantage is that tree recompilation takes time quadratic in the number of traces. We believe that the cost of recompiling a trace tree every time a branch is added would be prohibitive. That problem might be mitigated by recompiling only at certain points, or only for very hot, stable trees. </P>

<P>In the future, multicore hardware is expected to be common, making background tree recompilation attractive. In a closely related project (13) background recompilation yielded speedups of up to 1.25x on benchmarks with many branch traces.We plan to applythis techniquetoTraceMonkey as futurework. </P>
</Sect>

<Sect>
<H6>6.3 Trace Recording </H6>

<P>The job of the trace recorder is to emit LIR with identical semantics to the currently running interpreter bytecode trace.Agood implementation should have low impact on non-tracing interpreter performance and a convenient way for implementers to maintain semantic equivalence. </P>

<P>In our implementation, the only direct modiﬁcation to the interpreter is a call to the trace monitor at loop edges. In our benchmark results (see Figure 12) the total time spent in the monitor (for all activities) is usually less than 5%, so we consider the interpreter impact requirement met. Incrementing the loop hit counter is expensivebecause it requires us to look up the loop in the trace cache, but we have tuned our loops to become hot and trace very quickly (on the second iteration). The hit counter implementation could be improved, which might give us a small increase in overall performance, as well as more ﬂexibility with tuning hotness thresholds. Once a loop is blacklisted we never call into the trace monitor for that loop (see Section 3.3). </P>

<P>Recording is activated by a pointer swap that sets the interpreter’s dispatch table to call a single “interrupt” routine for every bytecode. The interrupt routine ﬁrst callsa bytecode-speciﬁc recording routine. Then, it turns off recording if necessary (e.g., the trace ended). Finally, it jumps to the standard interpreter byte-code implementation. Some bytecodes haveeffects on the type map that cannot be predicted before executing the bytecode (e.g., calling String.charCodeAt, which returns an integer or NaN if the indexargumentisoutof range).For these,we arrangeforthe interpreter to call into the recorder again after executing the bytecode. Since such hooks are relatively rare, we embed them directly into the interpreter, with an additional runtime check to see whether a recorder is currently active. </P>

<P>While separating the interpreter from the recorder reduces individual code complexity,it also requires careful implementation and extensive testing to achieve semantic equivalence. </P>

<P>In some cases achieving this equivalence is difﬁcult since SpiderMonkeyfollows a fat-bytecode design, which was found to be beneﬁcial to pure interpreter performance. </P>

<P>In fat-bytecode designs, individual bytecodes can implement complex processing (e.g., the getprop bytecode, which implements fullJavaScript propertyvalue access, includingspecial cases for cached and dense array access). </P>

<P>Fat bytecodes have two advantages: fewer bytecodes means lower dispatch cost, and bigger bytecode implementations give the compiler more opportunities to optimize the interpreter. </P>

<P>Fat bytecodes are a problem for TraceMonkey because they require the recorder to reimplement the same special case logic in the same way. Also, the advantages are reduced because (a) dispatch costs are eliminated entirely in compiled traces, (b) the traces contain only one special case, not the interpreter’s large chunk of code, and (c)TraceMonkeyspends less time running the base interpreter. </P>

<P>Oneway wehave mitigated these problemsisby implementing certain complex bytecodes in the recorder as sequences of simple bytecodes. Expressingthe original semanticsthiswayisnottoodifﬁcult, and recording simple bytecodes is much easier. This enables usto retaintheadvantagesoffat bytecodes whileavoiding someof their problems for trace recording. This is particularly effective for fat bytecodes that recurse back into the interpreter, for example to convert an object into a primitive value by invoking a well-known method on the object, since it lets us inline this function call. </P>

<P>Itis importantto notethatwesplitfat opcodesinto thinneropcodes only during recording. When running purely interpretatively </P>

<P>(i.e. code that has been blacklisted), the interpreter directly and efﬁcientlyexecutes thefat opcodes. </P>
</Sect>

<Sect>
<H6>6.4 Preemption </H6>

<P>SpiderMonkey, like manyVMs, needs to preempt the user program periodically. The main reasons are to prevent inﬁnitely looping scripts from locking up the host system and to schedule GC. </P>

<P>In the interpreter, this had been implemented by setting a “preempt now” ﬂag that was checked on every backward jump. This strategy carriedover intoTraceMonkey: theVM insertsa guard on the preemption ﬂag atevery loop edge.We measured less thana 1% increase in runtime on most benchmarks for this extra guard. In practice, the cost is detectable only for programs with very short loops. </P>

<P>We tested and rejected a solution that avoided the guards by compiling the loop edge as an unconditional jump, and patching the jump target to an exit routine when preemption is required. This solution can make the normal case slightly faster, but then preemption becomes very slow. The implementation was also very complex, especially tryingto restartexecution afterthe preemption. </P>
</Sect>

<Sect>
<H6>6.5 Calling External Functions </H6>

<P>Like most interpreters, SpiderMonkeyhas a foreign function inter-face(FFI)thatallowsittocallCbuiltinsandhostsystem functions (e.g., web browser control and DOM access). The FFI has a standard signaturefor JS-callable functions,thekeyargumentof which is an array of boxed values. External functions called through the FFI interact with the program state through an interpreter API (e.g., to read a property from an argument). There are also certain interpreterbuiltinsthatdonotusetheFFI,but interactwiththe program state in the same way, such as the CallIteratorNext function used with iterator objects.TraceMonkey must support this FFI in order to speed up code that interacts with the host system inside hot loops. </P>

<P>Callingexternal functions fromTraceMonkeyis potentially difﬁcult because traces do not update the interpreter state until exiting. In particular, external functions may need the call stack or the globalvariables,but theymaybe outof date. </P>

<P>For the out-of-date call stack problem, we refactored some of the interpreter API implementation functions to re-materialize the interpreter call stack on demand. </P>

<P>We developed a C++ static analysis and annotated some interpreter functions in order to verify that the call stack is refreshed at any point it needs to be used. In order to access the call stack, a function must be annotated as either FORCESSTACK or REQUIRESSTACK. These annotations are also requiredin orderto call REQUIRESSTACKfunctions, which are presumed to access the call stack transitively. FORCESSTACK is a trusted annotation, applied to only5functions, that means the function refreshes the call stack. REQUIRESSTACK is an untrusted annotation that means the function may only be called if the call stack has already been refreshed. </P>

<P>Similarly, we detect when host functions attempt to directly read or write globalvariables, and force the currently running trace to side exit. This is necessary since we cache and unbox global variables into the activation record during trace execution. </P>

<P>Since both call-stack access and global variable access are rarely performedbyhost functions, performanceis not signiﬁcantly affectedby these safety mechanisms. </P>

<P>Another problemis thatexternal functions can reenterthe interpreter by calling scripts, which in turn again might want to access the call stack or globalvariables.Toaddress this problem, we made theVMsetaﬂagwheneverthe interpreterisreenteredwhileacompiled trace is running. </P>

<P>Everycalltoanexternal functionthen checksthisﬂagandexits the trace immediately after returning from theexternal function call ifitis set. There are many external functionsthat seldomornever reenter, and they can be called without problem, and will cause trace exit only if necessary. </P>

<P>The FFI’s boxed value array requirement has a performance cost, so we deﬁned a new FFI that allows C functions to be annotated with their argument types so that the tracer can call them directly, without unnecessary argument conversions. </P>

<P>Currently, we do not support calling native property get and set override functions or DOM functions directly from trace. Support is planned future work. </P>
</Sect>
</Sect>

<Sect>
<Sect>
<H6>6.6 Correctness </H6>

<P>During development, we had access to existing JavaScript test suites, but most of them were not designed with tracing VMs in mind and contained few loops. </P>

<P>One tool that helped us greatly was Mozilla’s JavaScript fuzz tester, JSFUNFUZZ, which generates random JavaScript programs by nesting random language elements. We modiﬁed JSFUNFUZZ to generate loops, and also to test more heavily certain constructs we suspectedwould reveal ﬂawsin our implementation.Forexample,we suspectedbugsinTraceMonkey’shandlingof type-unstable Figure 11. Fraction of dynamic bytecodes executed by interpreter and on native traces. The speedup vs. interpreter is shown in parentheses next to each test. The fraction of bytecodes executed while recording is too small to see in this ﬁgure, except for crypto-md5, where fully 3% of bytecodes are executed while recording. In most of the tests, almost all the bytecodes are executed by compiled traces. Three of the benchmarks are not traced at all and run in the interpreter. </P>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_7.jpg"/>
</Figure>

<P>loops and heavily branching code, and a specialized fuzz tester indeed revealed several regressions which we subsequently corrected. </P>

<P>7. Evaluation </P>

<P>We evaluated our JavaScript tracing implementation using Sun-Spider, the industry standard JavaScript benchmark suite. SunSpider consists of 26 short-running (less than 250ms, average 26ms) JavaScript programs. This is in stark contrast to benchmark suites such as SpecJVM98 (3) used to evaluate desktop and server Java VMs. Manyprograms in those benchmarks use large data sets and executefor minutes.The SunSpider programs carryoutavarietyof tasks, primarily 3d rendering, bit-bashing, cryptographic encoding, mathkernels, and string processing. </P>

<P>All experiments were performed on a MacBook Pro with 2.2 GHz Core2processor and2GB RAM running MacOS 10.5. </P>

<P>Benchmark results. The main question is whether programs runfaster with tracing.For this, we ran the standard SunSpider test driver, which starts a JavaScript interpreter, loads and runs each program once for warmup, then loads and runs each program 10 times and reports theaverage time takenby each.We ran4different conﬁgurations for comparison: (a) SpiderMonkey, the baseline interpreter, (b)TraceMonkey, (d) SquirrelFish Extreme (SFX), the call-threaded JavaScript interpreter used in Apple’s WebKit, and </P>

<P>(e) V8, the method-compiling JavaScript VM from Google. </P>

<P>Figure10showsthe relativespeedups achievedbytracing,SFX, andV8against the baseline (SpiderMonkey).Tracing achieves the best speedups in integer-heavy benchmarks, up to the 25x speedup on bitops-bitwise-and. </P>

<P>TraceMonkey is the fastest VM on 9 of the 26 benchmarks (3d-morph, bitops-3bit-bits-in-byte, bitops-bitwiseand, crypto-sha1, math-cordic, math-partial-sums, mathspectral-norm, string-base64, string-validate-input). </P>
</Sect>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_8.jpg"/>
</Figure>

<P>Figure 10. Speedup vs. a baseline JavaScript interpreter (SpiderMonkey) for our trace-based JIT compiler, Apple’s SquirrelFish Extreme inline threading interpreter and Google’sV8 JS compiler. Our system generates particularly efﬁcient code for programs that beneﬁt most from type specialization, which includes SunSpider Benchmark programs that perform bit manipulation.We type-specialize the codein question to use integer arithmetic, which substantially improves performance.For oneof the benchmark programs weexecute25 timesfaster than the SpiderMonkeyinterpreter, and almost5timesfaster thanV8 and SFX.Fora large numberof benchmarks all three VMs produce similar results.We performworst on benchmark programs that wedo not trace and insteadfall back onto the interpreter. This includes the recursive benchmarks access-binary-trees and control-flow-recursive, for which we currently don’tgenerate anynative code. </P>

<Sect>
<P>In particular, the bitops benchmarks are short programs that perform manybitwise operations, soTraceMonkey can cover the entire program with1or2traces that operate on integers.TraceMonkeyruns all the other programs in this set almost entirely as native code. </P>

<P>regexp-dna is dominated by regular expression matching, whichis implementedin all3VMsbya special regularexpression compiler. Thus, performance on this benchmark has little relation to the trace compilation approach discussed in this paper. </P>

<P>TraceMonkey’s smaller speedups on the other benchmarks can be attributed to a few speciﬁc causes: </P>

<L>
<LI>
<LI_Label>• </LI_Label>

<LI_Title>The implementation does not currently trace recursion, so TraceMonkey achieves a small speedup or no speedup on benchmarks that use recursion extensively: 3d-cube, 3draytrace, access-binary-trees, string-tagcloud, and controlflow-recursive. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>The implementation does not currently trace eval and some other functions implemented in C. Because date-formattofte and date-format-xparb use such functions in their main loops, we do not trace them. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>The implementation does not currently trace through regular expression replace operations. The replace function can be passed a function object used to compute the replacement text. Our implementation currently does not trace functions called as replace functions. The run time of string-unpack-code is dominated by such a replace call. </LI_Title>
</LI>
</L>

<L>
<LI>
<LI_Label>• </LI_Label>

<LI_Title>Two programs trace well, but have a long compilation time. access-nbody formsalarge numberof traces(81). crypto-md5 forms oneverylong trace.Weexpecttoimprove performance on this programs by improving the compilation speed of nanojit. </LI_Title>
</LI>

<LI>
<LI_Label>• </LI_Label>

<LI_Title>Some programs trace very well, and speed up compared to the interpreter,but are not asfast as SFX and/or V8, namely bitops-bits-in-byte, bitops-nsieve-bits, accessfannkuch, access-nsieve, and crypto-aes. The reason is not clear, but all of these programs have nested loops with small bodies, so we suspectthat the implementation has a relatively high cost for calling nested traces. string-fasta traces well,butitsruntimeis dominatedbystring processingbuiltins, which are unaffected by tracing and seem to be less efﬁcient in SpiderMonkeythan in the two other VMs. </LI_Title>
</LI>
</L>

<P>Detailed performance metrics. In Figure 11 we show the fraction of instructions interpretedand the fraction of instructions executed as native code. This ﬁgure shows that for manyprograms, we are able to execute almost all the code natively. </P>

<P>Figure 12 breaks down the total execution time into four activities: interpreting bytecodes while not recording, recording traces (including time taken to interpret the recorded trace), compiling traces to native code, and executing native code traces. </P>

<P>These detailed metrics allow us to estimate parameters for a simple model of tracing performance. These estimates should be considered very rough, as the values observed on the individual benchmarks have large standard deviations (on the order of the </P>
</Sect>

<P>Loops Trees Traces Aborts Flushes Trees/Loop Traces/Tree Traces/Loop Speedup </P>

<P>3d-cube 25 27 29 3 0 1.1 1.1 1.2 2.20x 3d-morph 5 8 8 2 0 1.6 1.0 1.6 2.86x 3d-raytrace 10 25 100 10 1 2.5 4.0 10.0 1.18x access-binary-trees 0 0 0 5 0 ---0.93x access-fannkuch 10 34 57 24 0 3.4 1.7 5.7 2.20x access-nbody 8 16 18 5 0 2.0 1.1 2.3 4.19x access-nsieve 3 6 8 3 0 2.0 1.3 2.7 3.05x bitops-3bit-bits-in-byte 2 2 2 0 0 1.0 1.0 1.0 25.47x bitops-bits-in-byte 3 3 4 1 0 1.0 1.3 1.3 8.67x bitops-bitwise-and 1 1 1 0 0 1.0 1.0 1.0 25.20x bitops-nsieve-bits 3 3 5 0 0 1.0 1.7 1.7 2.75x controlﬂow-recursive 0 0 0 1 0 ---0.98x crypto-aes 50 72 78 19 0 1.4 1.1 1.6 1.64x crypto-md5 4 4 5 0 0 1.0 1.3 1.3 2.30x crypto-sha1 5 5 10 0 0 1.0 2.0 2.0 5.95x date-format-tofte 3 3 4 7 0 1.0 1.3 1.3 1.07x date-format-xparb 3 3 11 3 0 1.0 3.7 3.7 0.98x math-cordic 2 4 5 1 0 2.0 1.3 2.5 4.92x math-partial-sums 2 4 4 1 0 2.0 1.0 2.0 5.90x math-spectral-norm 15 20 20 0 0 1.3 1.0 1.3 7.12x regexp-dna 2 2 2 0 0 1.0 1.0 1.0 4.21x string-base64 3 5 7 0 0 1.7 1.4 2.3 2.53x string-fasta 5 11 15 6 0 2.2 1.4 3.0 1.49x string-tagcloud 3 6 6 5 0 2.0 1.0 2.0 1.09x string-unpack-code 4 4 37 0 0 1.0 9.3 9.3 1.20x string-validate-input 6 10 13 1 0 1.7 1.3 2.2 1.86x </P>

<P>Figure 13. Detailed trace recording statistics for the SunSpider benchmark set. </P>

<Sect>
<P>mean).We exclude regexp-dna from the following calculations, because most of its time is spent in the regular expression matcher, which has much different performance characteristics from the other programs. (Note that this only makes a difference of about 10% in the results.) Dividing the total execution time in processor clock cycles by the number of bytecodes executed in the base interpreter shows that on average, a bytecode executes in about 35cycles. Native traces take about9 cycles per bytecode,a 3.9x speedup over the interpreter. </P>

<P>Using similar computations, we ﬁnd that trace recording takes about 3800 cycles per bytecode, and compilation 3150 cycles per bytecode. Hence, during recording and compiling the VM runs at 1/200 the speed of the interpreter. Because it costs 6950 cycles to compile a bytecode, and we save 26 cycles each time that code is run natively, we break even after running a trace 270 times. </P>

<P>The other VMs we compared with achieve an overall speedup of 3.0x relative to our baseline interpreter. Our estimated native code speedup of 3.9x is signiﬁcantly better. This suggests that our compilation techniques can generate more efﬁcient native code than anyother current JavaScript VM. </P>

<P>These estimates also indicate that our startup performance could be substantially better if we improved the speed of trace recording and compilation. The estimated 200x slowdown for recording and compilationisvery rough,andmaybe inﬂuencedbystartupfactors in the interpreter (e.g., caches that have not warmed up yet during recording). One observation supporting this conjecture is that in the tracer, interpreted bytecodes take about 180 cycles to run. Still, recording and compilation are clearly both expensive, and a better implementation, possibly including redesign of the LIR abstract syntax or encoding, would improve startup performance. </P>

<P>Our performance results conﬁrm that type specialization using trace trees substantially improves performance. We are able to outperform thefastestavailableJavaScript compiler (V8) and the fastestavailableJavaScript inline threaded interpreter (SFX) on9 of 26 benchmarks. </P>

<P>8. Related Work </P>

<P>Trace optimization for dynamic languages. The closest area of related work is on applying trace optimization to type-specialize dynamic languages. Existing work shares the idea of generating type-specialized code speculatively with guards along interpreter traces. </P>

<P>To our knowledge, Rigo’s Psyco (16) is the only published type-specializing trace compiler for a dynamic language (Python). Psyco does not attempt to identify hot loops or inline function calls. Instead, Psyco transforms loops to mutual recursion before running and traces all operations. </P>

<P>Pall’s LuaJIT is a Lua VM in development that uses trace compilation ideas. (1). There are no publications on LuaJITbut the creatorhastoldusthat LuaJIThasa similardesigntoour system,but will use a less aggressive type speculation (e.g., using a ﬂoating-point representation for all number values) and does not generate nested traces for nested loops. </P>

<P>General trace optimization. General trace optimization has a longer history that has treated mostly native code and typed languages like Java. Thus, these systems have focused less on type specialization and more on other optimizations. </P>

<P>Dynamo (7) by Bala et al, introduced native code tracing as a replacement for proﬁle-guided optimization (PGO).Amajor goal was to perform PGO online so that the proﬁle was speciﬁc to the current execution. Dynamo used loop headers as candidate hot traces,but did not try to create loop traces speciﬁcally. </P>

<P>Trace trees were originally proposed by Gal et al. (11) in the context of Java, a statically typed language. Their trace trees actually inlined parts of outer loops within the inner loops (because Figure 12. Fraction of time spent on major VM activities. The speedup vs. interpreter is shown in parentheses next to each test. Most programs where the VM spends the majority of its time running native code have a good speedup. Recording and compilation costs can be substantial; speeding up those parts of the implementation would improve SunSpider performance. </P>
<Figure>

<ImageData src="images/compressed.tracemonkey-pldi-09_img_9.jpg"/>
</Figure>

<P>inner loops become hot ﬁrst), leading to much greater tail duplication. </P>

<P>YETI, from Zaleski et al. (19) applied Dynamo-style tracing to Java in order to achieve inlining, indirect jump elimination, and other optimizations. Their primary focus was on designing an interpreter that could easily be gradually re-engineered as a tracing VM. </P>

<P>Suganuma et al. (18) described region-based compilation (RBC), a relativeof tracing.A regionis an subprogramworth optimizing that can include subsets of anynumber of methods. Thus, the compiler has more ﬂexibility and can potentially generate better code, but the proﬁling and compilation systems are correspondingly more complex. </P>

<P>Type specialization for dynamic languages. Dynamic language implementors have long recognized the importance of type specialization for performance. Most previouswork has focused on methods instead of traces. </P>

<P>Chambers et. al (9) pioneered the idea of compiling multiple versions of a procedure specialized for the input types in the language Self. In one implementation, they generated a specialized method online each timeamethodwas called with newinput types. In another, they used an ofﬂine whole-program static analysis to infer input types and constant receiver types at call sites. Interestingly, the two techniques produced nearly the same performance. </P>

<P>Salib (17) designeda type inference algorithm for Python based on the Cartesian ProductAlgorithm and used the results to specialize on types and translate the program to C++. </P>

<P>McCloskey (14) has work in progress based on a language-independent type inference that is used to generate efﬁcient C implementations of JavaScript and Python programs. </P>

<P>Native code generation by interpreters. The traditional interpreter design is a virtual machine that directly executes ASTs or machine-code-likebytecodes. Researchers haveshown howto generate native code with nearly the same structurebut better performance. </P>

<P>Call threading, also known as context threading (8), compiles methods by generating a native call instruction to an interpreter method for each interpreter bytecode.A call-return pair has been shown to be a potentially much more efﬁcient dispatch mechanism than the indirect jumps used in standard bytecode interpreters. </P>

<P>Inline threading (15) copies chunks of interpreter native code which implement the required bytecodes into a native code cache, thus acting asa simple per-method JIT compiler that eliminates the dispatch overhead. </P>

<P>Neither call threading nor inline threading perform type specialization. </P>

<P>Apple’s SquirrelFish Extreme (5) is a JavaScript implementation based on call threading with selective inline threading. Combined with efﬁcient interpreter engineering, these threading techniqueshavegivenSFXexcellent performanceonthe standardSun-Spider benchmarks. </P>

<P>Google’s V8 is a JavaScript implementation primarily based on inline threading, with call threading only for very complex operations. </P>

<P>9. Conclusions </P>

<P>This paper described how to run dynamic languages efﬁciently by recording hot traces and generating type-specialized native code. Our technique focuses on aggressively inlined loops, and for each loop, it generates a tree of native code traces representing the paths andvalue types through the loop observed at run time.We explained how to identify loop nesting relationships and generate nested traces in order to avoid excessive code duplication due to the many paths through a loop nest. We described our type specialization algorithm. We also described our trace compiler, which translates a trace from an intermediate representation to optimized native code in two linear passes. </P>

<P>Our experimental results show that in practice loops typically are entered with only a few different combinations of value types of variables. Thus, a small number of traces per loop is sufﬁcient to run a program efﬁciently. Our experiments also show that on programs amenable to tracing, we achieve speedups of 2x to 20x. </P>

<P>10. Future Work </P>

<P>Work is underway in a number of areas to further improve the performance of our trace-based JavaScript compiler.We currently do not trace across recursive function calls, but plan to add the support for this capabilityin the near term.We are alsoexploring adoption of the existing work on tree recompilation in the context of the presented dynamic compiler in order to minimize JIT pause times and obtain the bestof bothworlds,fast tree stitching as well as the improved code quality due to tree recompilation. </P>

<P>We also plan on adding support for tracing across regular expression substitutions using lambda functions, function applications and expression evaluation using eval. All these language constructs are currently executed via interpretation, which limits our performance for applications that use those features. </P>

<P>Acknowledgments </P>

<P>Parts of this effort have been sponsored by the National Science Foundation under grants CNS-0615443 and CNS-0627747, as well as by the California MICRO Program and industrial sponsor Sun Microsystems under Project No. 07-127. </P>

<P>The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding anycopyright annotation thereon. Anyopinions, ﬁndings, and conclusions or recommendations expressed here are those of the author and should not be interpreted as necessarily representing the ofﬁcial views, policies or endorsements, either expressed or implied, of the National Science foundation (NSF), anyother agencyof the U.S. Government, or anyof the companies mentioned above. </P>

<P>References </P>

<P>[1] LuaJIT 	roadmap 2008 -http://lua-users.org/lists/lua-l/200802/msg00051.html. </P>

<P>[2] Mozilla 	— Firefox web browser and Thunderbird email client http://www.mozilla.com. </P>

<P>[3] SPECJVM98 -http://www.spec.org/jvm98/. </P>

<P>[4] SpiderMonkey 	(JavaScript-C) Engine http://www.mozilla.org/js/spidermonkey/. </P>

<P>[5] Surﬁn’ Safari -Blog Archive -Announcing SquirrelFish Extreme http://webkit.org/blog/214/introducing-squirrelﬁsh-extreme/. </P>

<P>[6] A. Aho, R. Sethi, J. Ullman, and M. Lam. 	Compilers: Principles, techniques, and tools, 2006. </P>

<P>[7] V. Bala, E. Duesterwald, and S. Banerjia. 	Dynamo: A transparent dynamic optimization system. In Proceedings of theACM SIGPLAN Conference on Programming Language Design and Implementation, pages 1–12.ACM Press, 2000. </P>

<P>[8] M. Berndl, B.Vitale, M. Zaleski, and A. Brown. Context Threading: a Flexible and Efﬁcient DispatchTechnique forVirtual Machine Interpreters. In Code Generation and Optimization, 2005. CGO 2005. International Symposium on, pages 15–26, 2005. </P>

<P>[9] C. Chambers and D. Ungar. 	Customization: Optimizing Compiler Technology for SELF, a Dynamically-Typed O bject-Oriented Programming Language. In Proceedings of the ACM SIGPLAN 1989 Conference on Programming Language Design and Implementation, pages 146–160.ACMNewYork,NY, USA, 1989. </P>

<P>[10] A. Gal. 	Efﬁcient BytecodeVeriﬁcation and CompilationinaVirtual Machine Dissertation. PhD thesis, University Of California, Irvine, 2006. </P>

<P>[11] A. Gal, C.W. Probst, and M. Franz. 	HotpathVM: An effective JIT compiler for resource-constrained devices. In Proceedings of the International Conference on Virtual Execution Environments, pages 144–153.ACM Press, 2006. </P>

<P>[12] C. Garrett, J. Dean, D. Grove, and C. Chambers. 	Measurement and Application of Dynamic Receiver Class Distributions. 1994. </P>

<P>[13] J. Ha, M. R. Haghighat, S. Cong, and K. S. McKinley. 	Aconcurrent trace-based just-in-time compiler for javascript. Dept.of Computer Sciences,TheUniversityofTexasat Austin, TR-09-06, 2009. </P>

<P>[14] B. McCloskey. Personal communication. </P>

<P>[15] I. Piumarta andF. Riccardi. Optimizing direct threaded codeby selective inlining. In Proceedings of theACM SIGPLAN 1998 conference on Programming language design and implementation, pages 291– </P>

<P>300.ACMNewYork,NY, USA, 1998. </P>

<P>[16] A. Rigo. 	Representation-Based Just-In-time Specialization and the Psyco Prototype for Python. In PEPM, 2004. </P>

<P>[17] M. 	Salib. Starkiller: A Static Type Inferencer and Compiler for Python. In Master’s Thesis, 2004. </P>

<P>[18]T.Suganuma,T.Yasue, andT. Nakatani. 	ARegion-Based CompilationTechnique for Dynamic Compilers. ACMTransactions on Programming Languages and Systems (TOPLAS), 28(1):134–174, 2006. </P>

<P>[19] M. Zaleski, A. D. Brown, and K. Stoodley. 	YETI: A graduallY Extensible Trace Interpreter. In Proceedings of the International Conference on Virtual Execution Environments, pages 83–93. ACM Press, 2007. </P>
</Sect>
</Sect>
</Sect>
</Sect>
</Sect>
</Part>
</TaggedPDF-doc>
